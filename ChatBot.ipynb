{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "X4Mi-E4MpqRg",
        "aycGprVapxZ7",
        "wYKT8Hb-qVuj",
        "2_VL_JFerJZK",
        "KjXeRn-8s1FJ",
        "6h1jU4gwtg3T",
        "vmWjh69R92BT",
        "ncWfuuj4whE4",
        "W3QoyflP06f0",
        "1bqD8FgA9fQ3",
        "3WEfxeVR9kZ6",
        "tCeec0_59xn1",
        "DLjtz-_I-BNT",
        "FvevZPLr-Z5m",
        "5s-I1W-0iSqs",
        "8uneUrcBkL53",
        "eH0cVCGkySpI",
        "7J0TpNugyltM",
        "Fb4K4J_-yzTP"
      ],
      "authorship_tag": "ABX9TyPgWJQwG6DBFYP2gLjkPLME",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pravin691983/chatbot/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiL5FYx4yt5Q"
      },
      "source": [
        "# Overview\n",
        "\n",
        "In this project is implemented, compared and analyzed two models, retrieval-based & generative, that constitute the state of the art in neural machine translation applied to chatbots. \n",
        "One model, i.e. retrieval-based model is based on simple neural network and for other generative model implementation is done exclusively using Sequence to Sequence(LSTM Encoder Decoder) architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LFKXonYy9_g"
      },
      "source": [
        "# Problem Statements\n",
        "\n",
        "Businesses aim to improve customer experience and also reduce costs, by integrating the right conversational AI technology that, enables automatic messaging and conversation between computers and humans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSBUpH5U72p7"
      },
      "source": [
        "# Anatomy of Conversational AI\n",
        "We can define the chatbots into two categories, following are the two categories of chatbots:\n",
        "\n",
        "- **Rule-Based Approach** – In this approach, a bot is trained according to rules. Based on this a bot can answer simple queries but sometimes fails to answer complex queries.\n",
        "- **Self-Learning Approach** – These bots follow the machine learning approach which is rather more efficient and is further divided into two more categories.\n",
        "  - **Retrieval-Based Models** – In this approach, the bot retrieves the best response from a list of responses according to the user input.\n",
        "  - **Generative Models** – These models often come up with answers than searching from a set of answers which makes them intelligent bots as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Mi-E4MpqRg"
      },
      "source": [
        "# 1. Retrieval-Based Models Chat Bot\n",
        "\n",
        "\n",
        "In this Python project with source code, we are going to build a chatbot using deep learning techniques. The chatbot will be trained on the dataset which contains categories (intents), pattern and responses. \n",
        "\n",
        "We use a simple neural network to classify which category the user’s message belongs to and then we will give a random response from the list of responses using NLTK, Keras, Tensor Flow, Python etc.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aycGprVapxZ7"
      },
      "source": [
        "## 1.1: IMPORT LIBRARIES AND DATASETS\n",
        "\n",
        "Now we’ll be importing some libraries needed to load, process, and transform our data and then feed it into a deep learning network. Just remember to keep your JSON file in the same directory as your python file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3akH63Cp7FZ"
      },
      "source": [
        "# Mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hua1UX_qLyf"
      },
      "source": [
        "projectPath = '/content/drive/My Drive/Colab Notebooks/Modern AI Portfolio Builder/Chat Bot/'\n",
        "%cd /content/drive/My Drive/Colab Notebooks/Modern AI Portfolio Builder/Chat Bot/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iRdEXOLCn9N"
      },
      "source": [
        "print(projectPath + 'intents.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0evmst5y_G4f"
      },
      "source": [
        "# Install necessary packages\n",
        "\n",
        "!pip install nltk\n",
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1S9No26qQ3b"
      },
      "source": [
        "# Import the necessary packages\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import PIL\n",
        "import seaborn as sns\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYKT8Hb-qVuj"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N11HMvtp_h2Y"
      },
      "source": [
        "# Initilise Data\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyCHTpWEA-44"
      },
      "source": [
        "# Download wordnet & punkt\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XhuJaky_rBq"
      },
      "source": [
        "# load rules based intents\n",
        "data_file = open(projectPath + 'Data/intents.json').read()\n",
        "intents = json.loads(data_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7xwFb_rAsMZ"
      },
      "source": [
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        #tokenize each word\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "        #add documents in the corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhljWeKMq7HP"
      },
      "source": [
        "documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoDpltPzA3Qo"
      },
      "source": [
        "classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_VL_JFerJZK"
      },
      "source": [
        "## 1.2: PERFORM DATA CLEANUP AND FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqaH286iBMR7"
      },
      "source": [
        "# lemmatize, lower each word and remove duplicates\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "\n",
        "words = sorted(list(set(words)))\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")\n",
        "# classes = intents\n",
        "print (len(classes), \"classes\", classes)\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjXeRn-8s1FJ"
      },
      "source": [
        "## 1.3: TRAIN DEEP LEARNING MODEL FOR RETRIEVAL BASED CHAT BOT\n",
        "\n",
        "Now, we have to take the “tag” and “patterns” out of the file and store it in a list. We’ll also make a collection of unique words in the patterns to create a Bag of Words (BoW) vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMfnO5cABW4l"
      },
      "source": [
        "# create our training data\n",
        "training = []\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    print(\"Before lematize pattern_words\", pattern_words)\n",
        "    # lemmatize each word - create base word, in attempt to represent related words\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    print(\"After lematize pattern_words\", pattern_words)\n",
        "    # create our bag of words array with 1, if word match found in current pattern\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    \n",
        "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    \n",
        "    print(\"bag : \", bag)\n",
        "    training.append([bag, output_row])\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzNe39flmkRK"
      },
      "source": [
        "# create train and test lists. X - patterns, Y - intents\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Training data created\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qml7g2Sm6os"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qEmYdFDs-D2"
      },
      "source": [
        "# X_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmx3kYzstBBD"
      },
      "source": [
        "# train_y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h1jU4gwtg3T"
      },
      "source": [
        "## 1.4: BUILD DEEP NEURAL NETWORK RETRIEVAL BASED CHAT BOT MODEL \n",
        "\n",
        "Now that we’re done with data preprocessing, it’s time to build a model and feed our preprocessed data to it. The network architecture is not too complicated. We will be using Fully Connected Layers (FC layers) with two of them being hidden layers and one giving out the target probabilities. Hence, the last layer will be having a softmax activation.\n",
        "Feel free to mess around with the architecture and the numbers to get the model that suits your requirements. You could also choose to add a bit more steps into text preprocessing to get more out of the data. The more trial and error cycles you perform better will be your understanding of the architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF5moVeNoaTR"
      },
      "source": [
        "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
        "# equal to number of intents to predict output intent with softmax\n",
        "model_retrieval_based = Sequential()\n",
        "model_retrieval_based.add(Dense(128, input_shape=(len(X_train[0]),), activation='relu'))\n",
        "model_retrieval_based.add(Dropout(0.5))\n",
        "model_retrieval_based.add(Dense(64, activation='relu'))\n",
        "model_retrieval_based.add(Dropout(0.5))\n",
        "model_retrieval_based.add(Dropout(0.5))\n",
        "model_retrieval_based.add(Dense(len(y_train[0]), activation='softmax'))\n",
        "model_retrieval_based.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmWjh69R92BT"
      },
      "source": [
        "## 1.5: COMPILE AND TRAIN RETRIEVAL BASED CHAT BOT DEEP LEARNING MODEL\n",
        "\n",
        "All we have to do now is feed the data to this model and begin training. We will set our epochs to 200 and batch size to 8. Again, you can experiment with these numbers and find the right one for your data. After training, we will be saving it on the disk so that we can use the trained model in our Flask application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQgSsqnbBmq2"
      },
      "source": [
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model_retrieval_based.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHMSgbSlESZ2"
      },
      "source": [
        "# save the best model with least validation loss\n",
        "checkpointer = ModelCheckpoint(filepath = \"RetrievalBased_ChatBot_weights.hdf5\", verbose = 1, save_best_only = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QgrnnSrEZt3"
      },
      "source": [
        "#fitting and saving the model \n",
        "\n",
        "# history = model.fit(np.array(train_x), np.array(train_y), batch_size = 5, epochs = 200, verbose=1)\n",
        "\n",
        "# history = model.fit(np.array(train_x), np.array(train_y), batch_size = 5, epochs = 50, verbose=1, callbacks=[checkpointer])\n",
        "\n",
        "\n",
        "# history = model.fit(X_train, y_train, batch_size = 32, epochs = 2, validation_split = 0.05, callbacks=[checkpointer])\n",
        "history = model_retrieval_based.fit(np.array(X_train), np.array(y_train), batch_size = 5, epochs = 50, verbose=1, validation_split = 0.05, callbacks=[checkpointer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EA1hx5jaWiW"
      },
      "source": [
        "# model.save('RuledBased_ChatBot_model.h5', history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLgnlS7LElyB"
      },
      "source": [
        "# save the model architecture to json file for future use\n",
        "model_json = model_retrieval_based.to_json()\n",
        "with open(\"RetrievalBased_ChatBot_model.json\",\"w\") as json_file:\n",
        "  json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jga1Zfyu5XL"
      },
      "source": [
        "## 1.6: ASSESS TRAINED RETRIEVAL MODEL PERFORMANCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpeXDSrLBse7"
      },
      "source": [
        "from keras.models import load_model\n",
        "import json\n",
        "import random\n",
        "intents = json.loads(open(projectPath + 'Data/intents.json').read())\n",
        "words = pickle.load(open('words.pkl','rb'))\n",
        "classes = pickle.load(open('classes.pkl','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIhhPmUjgXMM"
      },
      "source": [
        "# model = load_model('RuledBased_ChatBot_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3-osNxAKH00"
      },
      "source": [
        "# data_file = open(projectPath + 'Data/intents.json').read()\n",
        "with open(projectPath + 'RetrievalBased_ChatBot_model.json', 'r') as json_file:\n",
        "    json_savedModel= json_file.read()\n",
        "\n",
        "# load the model architecture \n",
        "model_retrieval_based = tf.keras.models.model_from_json(json_savedModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u_-6kmYZWA8"
      },
      "source": [
        "# Load the model wieghts\n",
        "from pathlib import Path\n",
        "\n",
        "my_file = Path(projectPath + 'RetrievalBased_ChatBot_weights.hdf5')\n",
        "if my_file.is_file():\n",
        "    # file exists\n",
        "    model_retrieval_based.load_weights(projectPath + 'RetrievalBased_ChatBot_weights.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spMGRA-lZcnN"
      },
      "source": [
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model_retrieval_based.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D9F7PvnFLIU"
      },
      "source": [
        "# Evaluate the model\n",
        "\n",
        "result = model_retrieval_based.evaluate(X_test, y_test)\n",
        "print(\"Accuracy : {}\".format(result[1]))\n",
        "\n",
        "# Get the model keys \n",
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FcMP2cliatW"
      },
      "source": [
        "# Plot the training artifacts\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Retrieval Based Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_loss','val_loss'], loc = 'upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HV-erNv_pbQ"
      },
      "source": [
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZx-sf1y_t7P"
      },
      "source": [
        "epochs = range(len(accuracy))\n",
        "\n",
        "plt.plot(epochs, accuracy, 'bo', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\n",
        "plt.title('Retrieval Based Training and Validation Accuracy')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik7WQVFX_wbv"
      },
      "source": [
        "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Retrieval Based Training and Validation loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXzPITfnB-Kr"
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern - split words into array\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stem each word - create short form for word\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    # print(sentence_words)\n",
        "    return sentence_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTaPXJLFCA9W"
      },
      "source": [
        "def bow(sentence, words, show_details=True):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words - matrix of N words, vocabulary matrix\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                # assign 1 if current word is in the vocabulary position\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yIJNLnUBwCJ"
      },
      "source": [
        "def predict_class(sentence, model):\n",
        "    # filter out predictions below a threshold\n",
        "    p = bow(sentence, words,show_details=False)\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs2HR5xJCG8m"
      },
      "source": [
        "def getResponse(ints, intents_json):\n",
        "    list_of_intents = intents_json['intents']\n",
        "\n",
        "    if ints:\n",
        "      tag = ints[0]['intent']\n",
        "      # print('List of intents :', list_of_intents)\n",
        "      for i in list_of_intents:\n",
        "          if(i['tag']== tag):\n",
        "              result = random.choice(i['responses'])\n",
        "              break\n",
        "    else:\n",
        "      for i in list_of_intents:\n",
        "        if(i['tag']== 'noanswer'):\n",
        "          result = random.choice(i['responses'])\n",
        "          break\n",
        "\n",
        "    # print('getResponse result :', result)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPcaFAEICJoQ"
      },
      "source": [
        "def chatbot_response(msg):\n",
        "    ints = predict_class(msg, model_retrieval_based)\n",
        "    # print('predict_class response : ',ints)\n",
        "    res = getResponse(ints, intents)\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW5d5pKpCL1r"
      },
      "source": [
        "chatbot_response(\"hi\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV6rPz7Ys64b"
      },
      "source": [
        "chatbot_response(\"@%@#$%@$%!@#CSDF\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuHB3h28ontG"
      },
      "source": [
        "def testRetrivalModel(): \n",
        "  print(\"Welcome to the Bot Service! Let me know how can I help you?\")\n",
        "  while True:\n",
        "      request=input('User'+':')\n",
        "      if request=='Bye' or request =='bye':\n",
        "          print('Bot: Bye')\n",
        "          break\n",
        "      else:\n",
        "          print('Bot:',chatbot_response(request))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KztFMc4CoUC"
      },
      "source": [
        "testRetrivalModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu0N3UF8vP_o"
      },
      "source": [
        "# 2. Generative Content Based Chat Bot\n",
        "\n",
        "In the above article, the responses were fixed and the machine learning helped to select the correct response given in the user’s question. But here, we are not going to select from pre-defined responses but instead, we will generate a response based on the training corpus. We are going to use the encoder-decoder (seq2seq) model for this approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA_vnnXyMfkq"
      },
      "source": [
        "## 2.1: IMPORT LIBRARIES AND DATASETS\n",
        "\n",
        "To train a Deep learning NLP network in supervised mode, we need labeled dataset, so as the chatbot seq2seq model will learn how to process questions and generate corresponding answers. Here some datasets that we can use :\n",
        "\n",
        "- Question-Answer Dataset: http://www.cs.cmu.edu/~ark/QA-data/\n",
        "- The WikiQA corpus: https://www.microsoft.com/en-us/download/confirmation.aspx?id=52419\n",
        "- Yahoo Language Data: https://webscope.sandbox.yahoo.com/catalog.php?datatype=l\n",
        "- ConvAI2 Dataset: http://convai.io/data\n",
        "- Open dialogue dataset (Microsoft/Maluuba): booking flights and a hotel. https://datasets.maluuba.com/Frames\n",
        "- Cornell Movie — Dialogs Corpus: https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hku0clu50Qvd"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Modern AI Portfolio Builder/Chat Bot/Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_okupEtqoWK"
      },
      "source": [
        "# import data for generic content based chat bot\n",
        "import os\n",
        "import yaml\n",
        "from tensorflow.keras import layers , activations , models , preprocessing, utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHuf0OqW0daj"
      },
      "source": [
        "# unzip download datayes\n",
        "!unzip chatbot_nlp.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ughhc4c0dAy"
      },
      "source": [
        "# prepare data\n",
        "\n",
        "dir_path = 'chatbot_nlp/data'\n",
        "files_list = os.listdir(dir_path + os.sep)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncWfuuj4whE4"
      },
      "source": [
        "## 2.2: VISUALIZE DATA AND PLOT LABELS (TBD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRNlJn9mwtKQ"
      },
      "source": [
        "# Plot bar chart to outline how many samples (images) are present per emotion\n",
        "\n",
        "# plt.figure(figsize = (10,10))\n",
        "# sns.barplot(x = facialexpression_df.emotion.value_counts().index, y = facialexpression_df.emotion.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpZho3kaw0k8"
      },
      "source": [
        "## 2.3: PERFORM DATA PREPARATION AND FEATURE ENGINEERING\n",
        "\n",
        "In our example, we use Cornell Movie — Dialogs Corpus that contains 220,579 conversational exchanges (304,713 utterances) between 10,292 pairs (involving 9,035 characters) extracted from 617 movies:\n",
        "\n",
        "Here one of the conversations from the data set:\n",
        "- Mike: \n",
        "  -\"Drink up, Charley. We're ahead of you.\"\n",
        "-Charley: \n",
        " -\"I'm not thirsty.\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPQTmmoll_X4"
      },
      "source": [
        " **Cleaning**:\n",
        " First, for the 2 files we get “Questions” and “Answers”, we must proceed to the cleaning, by replacing short form terms by their corresponding long terms:\n",
        "\n",
        " **Filtering**:\n",
        "Remove infrequent words that appear time to time, by counting words appearance for (less than a certain threshold, example 20), and replace it by a tag <OUT>.\n",
        "\n",
        "**Padding**:\n",
        "For the seq-2seq model, Questions, and answers sentences must have the same length, that why we apply padding technique by adding a term “PAD” when the sentence is shorter than the fixed initial length.\n",
        "\n",
        "**Tokenizing**\n",
        "Knowing that deep learning models understand only mathematics and numbers, the input word sequences must be encoded into a vector of numbers before feeding the Seq2Seq model. We use a two-step process to convert text into numbers that can be used in a neural network.\n",
        "The first step is Tokenizing that converts text-words into integer-tokens, by splitting the text into smaller parts (words and punctuations) called tokens, creating 2 dictionaries, one for “Questions” and another for “Answers” because their vocabularies are different and adding start <SOS> and end <EOS> tokens at the beginning and end of each utterance.\n",
        "\n",
        "**Word Embedding (Encoding corpus words)**\n",
        "The second step is to convert integer-tokens (words) into vectors of floating-point numbers. Many methods like Bag-of-words (e.g. TF-IDF or Count Vectorize), LDA, LSA or Word Embedding. The last one, Word Embedding, is recommended since it does not suffer from drawbacks like “high dimensional vector” that grow with the corpus size.\n",
        "Word Embedding encodes every word using a pre-defined and fixed vector space of N dimensions (E.g N=300), regardless of the size of the corpus. The word vector encodes the semantic relationship between words. Words have similar meaning if their vectors are closed (e.g using cosine similarity)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W5DexnMzD1A"
      },
      "source": [
        "# Load conversion in form of questions & answers\n",
        "questions = list()\n",
        "answers = list()\n",
        "\n",
        "for filepath in files_list:\n",
        "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(stream)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len( con ) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[ 1 : ]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append( ans )\n",
        "        elif len( con )> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq-1daG80LkV"
      },
      "source": [
        "print(len(questions))\n",
        "questions[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqLoD1hN0bWd"
      },
      "source": [
        "print(len(answers))\n",
        "answers[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAcJiDxTzQml"
      },
      "source": [
        "# Tag answers\n",
        "answers_with_tags = list()\n",
        "for i in range( len( answers ) ):\n",
        "    if type( answers[i] ) == str:\n",
        "        #print(\"answers[i]\", answers[i])\n",
        "        answers_with_tags.append( answers[i] )\n",
        "    else:\n",
        "        #print(\"questions[i]\", questions[i])\n",
        "        questions.pop( i )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbgXAh6C0i8H"
      },
      "source": [
        "print(len(answers_with_tags))\n",
        "answers_with_tags[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VCJw2zDzduw"
      },
      "source": [
        "# Prepare answers list with start and end tag description\n",
        "answers = list()\n",
        "for i in range( len( answers_with_tags ) ) :\n",
        "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
        "    #print('<START> ' + answers_with_tags[i] + ' <END>')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXnSycYE0wdA"
      },
      "source": [
        "print(len(answers))\n",
        "answers[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpbkS-DgznbH"
      },
      "source": [
        "# Tokenize questions & answers\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( questions + answers )\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14rwFxfR01TE"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPOZLBXf7O9f"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "\n",
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append( word )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTy6WawI1MJK"
      },
      "source": [
        "vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c13bkr51e9y"
      },
      "source": [
        "def tokenize( sentences ):\n",
        "    tokens_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = re.sub( '[^a-zA-Z]', ' ', sentence )\n",
        "        tokens = sentence.split()\n",
        "        vocabulary += tokens\n",
        "        tokens_list.append( tokens )\n",
        "    return tokens_list , vocabulary\n",
        "\n",
        "#p = tokenize( questions + answers )\n",
        "#model = Word2Vec( p[ 0 ] ) \n",
        "\n",
        "#embedding_matrix = np.zeros( ( VOCAB_SIZE , 100 ) )\n",
        "#for i in range( len( tokenizer.word_index ) ):\n",
        "    #embedding_matrix[ i ] = model[ vocab[i] ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40HKKqVS88Rx"
      },
      "source": [
        "# encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
        "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
        "encoder_input_data = np.array( padded_questions )\n",
        "print( encoder_input_data.shape , maxlen_questions )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyYL2azM8-HA"
      },
      "source": [
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print( decoder_input_data.shape , maxlen_answers )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yff12wgs9CU9"
      },
      "source": [
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
        "decoder_output_data = np.array( onehot_answers )\n",
        "print( decoder_output_data.shape )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOTK8LFiYHtH"
      },
      "source": [
        "padded_answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS53f0spYNTw"
      },
      "source": [
        "onehot_answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svENy6BYUNf_"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qypnihwJUvLH"
      },
      "source": [
        "tokenized_questions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uru8AkkQUyFR"
      },
      "source": [
        "tokenized_answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lGJlFo9TXcL"
      },
      "source": [
        "questions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz6UtJmoTY8Y"
      },
      "source": [
        "answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_RopubXTB0T"
      },
      "source": [
        "encoder_input_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb_GEOwvFq-h"
      },
      "source": [
        "encoder_input_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYdwQPKeTFW0"
      },
      "source": [
        "decoder_input_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmZpPv3lFtNt"
      },
      "source": [
        "decoder_input_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEFw1zZUTH1H"
      },
      "source": [
        "decoder_output_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ehfiN68FvI1"
      },
      "source": [
        "decoder_output_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9ZR1UzkPb16"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def data_spliter(encoder_input_data, decoder_input_data, test_size1=0.2, test_size2=0.3):\n",
        "  \n",
        "  en_train, en_test, de_train, de_test = train_test_split(encoder_input_data, decoder_input_data, test_size=test_size1)\n",
        "  en_train, en_val, de_train, de_val = train_test_split(en_train, de_train, test_size=test_size2)\n",
        "  \n",
        "  return en_train, en_val, en_test, de_train, de_val, de_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixnpJ89ADt67"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(decoder_input_data, decoder_output_data, test_size = 0.2)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(encoder_input_data, decoder_output_data, test_size = 0.2)\n",
        "\n",
        "en_train, en_val, en_test, de_train, de_val, de_test = data_spliter(encoder_input_data, decoder_input_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkBpEjumxTXR"
      },
      "source": [
        "## 2.4: BUILD AND TRAIN DEEP LEARNING MODEL FOR GENERIC CONTENT BASED CHAT BOT\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRpnDPcumGf7"
      },
      "source": [
        "**Recurrent Neural network (RNN)**\n",
        "**RNN** is a deep network that extracts temporal features while processing sequences of inputs like text, audio or video. It’s used when we need history/context to be able to provide the output based on previous inputs, like for video tracking, Image captioning, Speech-to-text, Translation, Stock forecasting, etc.\n",
        "\n",
        "RNN neuron uses its internal memory to maintain information about the previous inputs and update the hidden states accordingly, which allows them to make predictions for every element of a sequence.\n",
        "\n",
        "RNNs have shown great success in many NLP tasks, the most used type of RNN are LSTMs, that perform very well at capturing long-term dependencies than RNNs can do (due to the Vanishing gradient problem). GRU is a newer version of RNN with a less complex structure (fewer parameters) than LSTM, its training is a bit faster and need less data, but may lead to lower results.\n",
        "\n",
        "**Seq2Seq architecture and functioning:**\n",
        "Almost all task in NLP can be performed using a sequence to sequence mapping models: machine translation, summarization, question answering, and many more. An Encoder-Decoder model for recurrent neural networks is an architecture for sequence-to-sequence prediction problems in the field of natural language processing NLP, it takes a sequence as input and generates another sequence as output, It is comprised of two sub-modules :\n",
        "- **Encoder**: Process the input sequence to detect important patterns, in order to shrink it into a smaller fixed length “context vector”, this feature vector hold the information, that represents the input, which becomes the initial state to the first recurrent layer of the decoder part.\n",
        "- **Decoder**: generates a sequence of its own that represents the output. It gives the best closest match to the intended output during the training or to the actual input during the test or after Go live.\n",
        "\n",
        "\n",
        "To higher the performance and accuracy of the model, two additional algorithms can be used:\n",
        "- **Attention mechanism:**\n",
        "So as to perform well on long input or output sequences, we use Attention mechanism which tells the model the specific parts of the input sequence on which it must focus when decoding by providing a richer context from the encoder, instead of using only the raw “context vector”.\n",
        "-  **Beam search:** \n",
        "beam search is an algorithm that builds a search tree and tries to find the best path for a given number N on tree levels (limited set of nodes) in a greedy way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU8IuWh_7dij"
      },
      "source": [
        "# BUILD AND TRAIN DEEP LEARNING MODEL FOR GENERIC CONTENT BASED CHAT BOT\n",
        "\n",
        "# Dimension for embedding layer\n",
        "embedding_dimension = 200\n",
        "\n",
        "#Dimensionality\n",
        "dimensionality = 200 #256\n",
        "\n",
        "\n",
        "# Prepare encode input & embedding\n",
        "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, embedding_dimension , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( dimensionality , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "# Prepare decode input & embedding\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, embedding_dimension , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( dimensionality , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model_content_based = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "# model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "model_content_based.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBn5Ol76-GkB"
      },
      "source": [
        "## 2.5: COMPILE AND TRAIN GENERIC CONTENT BASED CHAT BOT DEEP LEARNING MODEL\n",
        "\n",
        "Our encoder model requires an input layer which defines a matrix for holding the one-hot vectors and an LSTM layer with some number of hidden states. Decoder model structure is almost the same as encoder’s but here we pass in the state data along with the decoder inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XsvPwdM7nsl"
      },
      "source": [
        "model_content_based.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# adam = tf.keras.optimizers.Adam(learning_rate = 0.0001, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
        "# model_1_facialKeyPoints.compile(loss = \"mean_squared_error\", optimizer = adam , metrics = ['accuracy'])\n",
        "# Check this out for more information on Adam optimizer: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCTWhp5Q-eMa"
      },
      "source": [
        "# save the best model with least validation loss\n",
        "checkpointer = ModelCheckpoint(filepath = \"ContentBase_ChatBot_weights.hdf5\", verbose = 1, save_best_only = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5E12zmR-d_X"
      },
      "source": [
        "#The batch size and number of epochs\n",
        "batch_size = 50 #10 # 50\n",
        "epochs = 150  # 600\n",
        "validation_split = 0.05\n",
        "\n",
        "history = model_content_based.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size = batch_size, epochs = epochs, validation_split = validation_split, callbacks=[checkpointer])\n",
        "\n",
        "# history = model_content_based.fit([en_train , de_train], decoder_output_data, batch_size = 50, epochs = 50, validation_split = 0.05, callbacks=[checkpointer])\n",
        "\n",
        "# history = model_content_based.fit([encoder_input_data , X_train], y_train, batch_size = 50, epochs = 2, validation_split = 0.05, callbacks=[checkpointer])\n",
        "# model_content_based.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150,  callbacks=[checkpointer] ) \n",
        "\n",
        "\n",
        "# history = model_1_facialKeyPoints.fit(X_train, y_train, batch_size = 32, epochs = 2, validation_split = 0.05, callbacks=[checkpointer])\n",
        "# Don't need to save this model\n",
        "# model.save( 'chatBot_model.h5' ) \n",
        "\n",
        "# en_train, en_val, en_test, de_train, de_val, de_test = data_spliter(encoder_input_data, decoder_input_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sTdYNbR-m65"
      },
      "source": [
        "# save the model architecture to json file for future use\n",
        "\n",
        "model_json = model_content_based.to_json()\n",
        "with open(\"ContentBase_ChatBot_model.json\",\"w\") as json_file:\n",
        "  json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNQ1AXYi0SgB"
      },
      "source": [
        "##2.6: ASSESS THE PERFORMANCE OF TRAINED GENERIC CONTENT BASED MODEL\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjCKSEpymTnj"
      },
      "source": [
        "Now, to handle an input that the model has not seen we will need a model that decodes step-by-step instead of using teacher forcing because the model we created only works when the target sequence is known. In the Generative chatbot application, we will not know what the generated response will be for input the user passes in. For doing this, we will have to build a seq2seq model in individual pieces. Let’s first build an encoder model with encoder inputs and encoder output states. We will do this with the help of the previously trained model.\n",
        "\n",
        "Next, we will need to create placeholders for decoder input states as we do not know what we need to decode or what hidden state we will get.\n",
        "\n",
        "Now we will create new decoder states and outputs with the help of decoder LSTM and Dense layer that we trained earlier.\n",
        "\n",
        "Finally, we have the decoder input layer, the final states from the encoder, the decoder outputs from the Dense layer of the decoder, and decoder output states which is the memory during the network from one word to the next. We can bring this all together now and set up the decoder model as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRq7lNJr0L1T"
      },
      "source": [
        "with open('ContentBase_ChatBot_model.json', 'r') as json_file:\n",
        "    json_savedModel= json_file.read()\n",
        "    \n",
        "# load the model architecture \n",
        "model_content_based = tf.keras.models.model_from_json(json_savedModel)\n",
        "model_content_based.load_weights('ContentBase_ChatBot_weights.hdf5')\n",
        "# model_content_based.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37VpRMW0EZPQ"
      },
      "source": [
        "# model_content_based.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TTXEl0c0V1E"
      },
      "source": [
        "# TODO: Fixed this issue, unable to calculate accuracy \n",
        "# score = model_content_based.evaluate([encoder_input_data + decoder_input_data], decoder_output_data)\n",
        "\n",
        "# print('Test Accuracy: {}'.format(score[1]))\n",
        "\n",
        "# en_train, en_val, en_test, de_train, de_val, de_test = data_spliter(encoder_input_data, decoder_input_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dONuadhG0ZGR"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCuqSUyRgai8"
      },
      "source": [
        "# Plot the training artifacts\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Generative Based Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_loss','val_loss'], loc = 'upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsfbU4zW0bzS"
      },
      "source": [
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noWcy6ub0eeu"
      },
      "source": [
        "epochs = range(len(accuracy))\n",
        "\n",
        "plt.plot(epochs, accuracy, 'bo', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\n",
        "plt.title('Generative Based Training and Validation Accuracy')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGXRlnML0gk4"
      },
      "source": [
        "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Generative Based Training and Validation loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhb1KRSt6v8L"
      },
      "source": [
        "At last, we will create a function that accepts our text inputs and generates a response using encoder and decoder that we created. In the function below, we pass in the NumPy matrix that represents our text sentence and we get the generated response back from it. I have added comments for almost every line of code for you to understand it quickly. What happens in the below function is this: 1.) We retrieve output states from the encoder 2.) We pass in the output states to the decoder (which is our initial hidden state of the decoder) to decode the sentence word by word 3.) Update the hidden state of decoder after decoding each word so that we can use previously decoded words to help decode new ones\n",
        "We will stop once we encounter ‘<END>’ token that we added to target sequences in our preprocessing task or we hit the maximum length of the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSpaqGLN7z3A"
      },
      "source": [
        "def make_inference_models():\n",
        "    \n",
        "    # first build an encoder model with encoder inputs and encoder output states.\n",
        "    # first build an encoder model with encoder inputs and encoder output states.\n",
        "    # encoder_inputs = model_content_based.input[0]\n",
        "    # encoder_outputs, state_h_enc, state_c_enc = model_content_based.layers[2].output\n",
        "    # encoder_states = [state_h_enc, state_c_enc]\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    #create placeholders for decoder input states\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    # create new decoder states and outputs with the help of decoder LSTM and Dense layer that we trained earlier.\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlhtOh9P766J"
      },
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "    # print(\"maxlen_questions\", maxlen_questions)\n",
        "    # print(\"tokenizer\", tokenizer)\n",
        "    # print(\"tokenizer.word_index\", tokenizer.word_index)\n",
        "    words = sentence.lower().split()\n",
        "    # print(\"words\", words)\n",
        "    tokens_list = list()\n",
        "    # print(\"Before tokens_list\", tokens_list)\n",
        "    for word in words:\n",
        "        if word in tokenizer.word_index :\n",
        "          # print(\"tokenizer word\", tokenizer.word_index[ word ])\n",
        "          tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "        else:\n",
        "          tokens_list.append( tokenizer.word_index[ \"out\" ] ) \n",
        "        # print(\"After tokens_list\", tokens_list)\n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocKTI6Rtvsda"
      },
      "source": [
        "enc_model, dec_model = make_inference_models()\n",
        "# enc_model , dec_model = make_inference_modelsEx()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt-sk_ED772c"
      },
      "source": [
        "def testGenerativeModel(): \n",
        "  print(\"Welcome to the Bot Service! Let me know how can I help you?\")\n",
        "  while True:\n",
        "      request=input('User'+':')\n",
        "      if request=='Bye' or request =='bye':\n",
        "          print('Bot: Bye')\n",
        "          break\n",
        "      else:\n",
        "          states_values = enc_model.predict( str_to_tokens( request ) )\n",
        "          empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "          empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "          stop_condition = False\n",
        "          decoded_translation = ''\n",
        "          while not stop_condition :\n",
        "            dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "            sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "            # print(\"sampled_word_index\", sampled_word_index)\n",
        "            if sampled_word_index == 1:\n",
        "              # print('Got end tag: Bye')\n",
        "              stop_condition = True\n",
        "              break\n",
        "\n",
        "            sampled_word = None\n",
        "            for word , index in tokenizer.word_index.items() :\n",
        "              if sampled_word_index == index :  \n",
        "                # print(\"word\", word)\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "              \n",
        "            if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "              stop_condition = True\n",
        "                  \n",
        "            empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "            empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "            states_values = [ h , c ]\n",
        "\n",
        "          # Print Respone\n",
        "          print( decoded_translation )  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6wyeXc7iaDL"
      },
      "source": [
        "testGenerativeModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI3--3Pf7UJ0"
      },
      "source": [
        "## 3. Future scope vs limitation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KUW9uTSmbj-"
      },
      "source": [
        "Here we used a very small dataset and got an accuracy of around 20%. In the future for a larger dataset, the model might give better accuracy. The limitation of using this approach for creating chatbots is that we need a very large dataset to give the best responses to the user as we can see in the above output that chatbot does not give the right responses in some cases because of a smaller dataset.\n",
        "A similar task we can do with the above-shown approach is Machine Translation. The below article shows how we can use the seq2seq model to perform Machine Translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RSIUQq15mhv"
      },
      "source": [
        "# Conclusion :\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUXTkyCWmgQd"
      },
      "source": [
        "- The rule-based & retrieval-based approaches are the most used nowadays due to its effectiveness at the time of maintaining a close-domain conversation. \n",
        "- However the generative-based models, on the other hand, arise as a powerful alternative in the sense that they can handle better an open topic conversation.\n",
        "- Add more datasets to help it learn better from more conversations. This can help improve its conversation skills and help it give a better variety of responses to queries.\n",
        "- The Seq2Seq model allows making a more realistic and human chatbot, the Dataset is also a crucial element in this equation, the larger and more diversified it is, the best is the user experience and perception.\n",
        "- The hybrid approach is also possible to develop a chatbot that is robust, reliable and scalable. This approach not only increases the quality, performance and accuracy of the chatbot but will also be more reliable in nature while handling real-time scenarios."
      ]
    }
  ]
}